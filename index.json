[{"content":"环境准备 准备一台 linux 物理机 安装对应的 packer 软件 安装对应的 qemu kvm 软件 镜像构建 分别将以下 hcl 文件以及 user-data 文件放置在提前准备好的目录当中\nPacker Hcl 文件 packer { required_version = \u0026#34;\u0026gt;= 1.7.0, \u0026lt; 2.0.0\u0026#34; required_plugins { qemu = { source = \u0026#34;github.com/hashicorp/qemu\u0026#34; version = \u0026#34;\u0026gt;= 1.0.0, \u0026lt; 2.0.0\u0026#34; } } } variable \u0026#34;vm_name\u0026#34; { type = string default = \u0026#34;ubuntu-2004.qcow2\u0026#34; } source \u0026#34;qemu\u0026#34; \u0026#34;test\u0026#34; { iso_url = \u0026#34;https://releases.ubuntu.com/20.04/ubuntu-20.04.5-live-server-amd64.iso\u0026#34; iso_checksum = \u0026#34;5035be37a7e9abbdc09f0d257f3e33416c1a0fb322ba860d42d74aa75c3468d4\u0026#34; vm_name = var.vm_name output_directory = \u0026#34;output\u0026#34; http_directory = \u0026#34;http\u0026#34; boot_wait = \u0026#34;2s\u0026#34; boot_command = [ \u0026#34;\u0026lt;esc\u0026gt;\u0026lt;esc\u0026gt;\u0026lt;esc\u0026gt;\u0026#34;, \u0026#34;\u0026lt;enter\u0026gt;\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;/casper/vmlinuz \u0026#34;, \u0026#34;initrd=/casper/initrd \u0026#34;, \u0026#34;autoinstall ds=nocloud-net;s=http://{{.HTTPIP}}:{{.HTTPPort}}/\u0026#34;, \u0026#34;\u0026lt;enter\u0026gt;\u0026#34; ] shutdown_command = \u0026#34;echo \u0026#39;packer\u0026#39; | sudo -S shutdown -P now\u0026#34; headless = true format = \u0026#34;qcow2\u0026#34; accelerator = \u0026#34;kvm\u0026#34; qemu_binary = \u0026#34;/usr/libexec/qemu-kvm\u0026#34; qemu_img_args { convert = [ \u0026#34;-m\u0026#34;, \u0026#34;8\u0026#34; ] } ssh_username = \u0026#34;root\u0026#34; ssh_password = \u0026#34;Root123!\u0026#34; ssh_timeout = \u0026#34;30m\u0026#34; vnc_bind_address = \u0026#34;0.0.0.0\u0026#34; cpus = 8 memory = 8192 net_device = \u0026#34;virtio-net\u0026#34; disk_size = \u0026#34;100G\u0026#34; disk_compression = true } build { name = \u0026#34;ubuntu20-04\u0026#34; sources = [ \u0026#34;source.qemu.test\u0026#34; ] provisioner \u0026#34;shell\u0026#34; { pause_before = \u0026#34;20s\u0026#34; inline = [\u0026#34;echo \u0026#39;build success\u0026#39;\u0026#34;] } } Ubuntu auto install user-data 文件 #cloud-config autoinstall: version: 1 early-commands: - hostnamectl set-hostname ubuntu # update hostname even for the installer environment - dhclient # re-register the updated hostname identity: # hostname of the system hostname: ubuntu # root doesn\u0026#39;t work username: ubuntu # ubuntu password: \u0026#34;$6$FhcddHFVZ7ABA4Gi$MhQrLRAMZI65UOGGwxyCYRgolj13tIHC3/MRfyQQlP4nD9jgIdn63Ol2qlO3I8I/Gfdcsg7k58dTYOzz3LeqJ.\u0026#34; ssh: install-server: true allow-pw: true user-data: timezone: Asia/Shanghai disable_root: false ssh_pwauth: true users: - name: root lock_passwd: false plain_text_passwd: Root123! - name: ubuntu lock_passwd: false plain_text_passwd: Root123! sudo: ALL=(ALL) NOPASSWD:ALL network: version: 2 ethernets: zz-all-en: match: name: \u0026#34;en*\u0026#34; dhcp4: true dhcp-identifier: mac zz-all-eth: match: name: \u0026#34;eth*\u0026#34; dhcp4: true dhcp-identifier: mac keyboard: layout: en variant: us locale: en_US storage: swap: size: 0 layout: name: direct packages: - bc - cloud-init - git - curl - wget - openssl - vim late-commands: - sed -i -e \u0026#39;s/^#\\?PasswordAuthentication.*/PasswordAuthentication yes/g\u0026#39; /target/etc/ssh/sshd_config - sed -i -e \u0026#39;s/^#\\?PermitRootLogin.*/PermitRootLogin yes/g\u0026#39; /target/etc/ssh/sshd_config - sed -i \u0026#39;s/^#*\\(send dhcp-client-identifier\\).*$/\\1 = hardware;/\u0026#39; /etc/dhcp/dhclient.conf - echo \u0026#39;ubuntu ALL=(ALL) NOPASSWD:ALL\u0026#39; \u0026gt; /target/etc/sudoers.d/ubuntu - curtin in-target --target=/target -- chmod 440 /etc/sudoers.d/ubuntu 执行 Packer 打包命令 packer build qemu.pkr.hcl 参考文档 Install Packer Ubuntu Automatic installation ","permalink":"https://yanhuan0802.github.io/posts/packer-qemu-ubuntu2004/","summary":"环境准备 准备一台 linux 物理机 安装对应的 packer 软件 安装对应的 qemu kvm 软件 镜像构建 分别将以下 hcl 文件以及 user-data 文件放置在提前准备好的目录当中\nPacker Hcl 文件 packer { required_version = \u0026#34;\u0026gt;= 1.7.0, \u0026lt; 2.0.0\u0026#34; required_plugins { qemu = { source = \u0026#34;github.com/hashicorp/qemu\u0026#34; version = \u0026#34;\u0026gt;= 1.0.0, \u0026lt; 2.0.0\u0026#34; } } } variable \u0026#34;vm_name\u0026#34; { type = string default = \u0026#34;ubuntu-2004.qcow2\u0026#34; } source \u0026#34;qemu\u0026#34; \u0026#34;test\u0026#34; { iso_url = \u0026#34;https://releases.ubuntu.com/20.04/ubuntu-20.04.5-live-server-amd64.iso\u0026#34; iso_checksum = \u0026#34;5035be37a7e9abbdc09f0d257f3e33416c1a0fb322ba860d42d74aa75c3468d4\u0026#34; vm_name = var.vm_name output_directory = \u0026#34;output\u0026#34; http_directory = \u0026#34;http\u0026#34; boot_wait = \u0026#34;2s\u0026#34; boot_command = [ \u0026#34;\u0026lt;esc\u0026gt;\u0026lt;esc\u0026gt;\u0026lt;esc\u0026gt;\u0026#34;, \u0026#34;\u0026lt;enter\u0026gt;\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;/casper/vmlinuz \u0026#34;, \u0026#34;initrd=/casper/initrd \u0026#34;, \u0026#34;autoinstall ds=nocloud-net;s=http://{{.","title":"使用 Packer QEMU 插件构建自定义 Ubuntu 20.04 操作系统镜像"},{"content":" 本文介绍基于 kubeadm + 内地网络 进行 k8s 集群部署。 本文介绍搭建单 master 节点 k8s 集群，适用于开发及测试，一般不适用生产环境。 环境准备 主机配置要求 k8s集群要求机器最低配置为 2CPU 2GRAM 关闭防火墙、交换分区、selinux # 防火墙 systemctl status firewalld # 如果没有firewalld.service，执行如下命令 sudo ufw status sudo ufw disable # 关闭swap swapoff -a cat /etc/fstab | grep swap # 删除或注释掉带有swap关键字的行 vi /etc/fstab # selinux # 检查 结果为Disabled即为关闭状态 apt update -y apt install -y selinux-utils getenforce # 如果selinux是启用状态，则需要永久关闭，关闭后重启机器方可生效 sed -i \u0026#39;s/SELINUX=enforcing/SELINUX=disabled/\u0026#39; /etc/selinux/config reboot 应用安装 安装容器运行时（CR） 允许 iptables 检查桥接流量 # 确保overlay br_netfilter内核模块加载 sudo modprobe overlay sudo modprobe br_netfilter cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF # 为了Linux 节点上的 iptables 能够正确地查看桥接流量，需要确保在sysctl 配置中将 net.bridge.bridge-nf-call-iptables 设置为 1 # 设置所需的 sysctl 参数，参数在重新启动后保持不变 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF # 应用 sysctl 参数而不重新启动 sudo sysctl --system 可以选择 Docker、containerd、或者 CRI-O 作为容器运行时，本文只介绍 docker 以及 containerd ，安装以下两种 CR 之中的一个即可。 Docker安装 软件包选择 docker.io 是 ubuntu 团队维护的一个包，所有的依赖包由 ubuntu 统一管理，每个外部依赖项都是一个单独的包，可以并且将被独立更新，更符合包管理的理念，也更规范，可以尽可能的保证系统和多个软件的运行稳定，适合作为多软件协同运行的服务器和个人桌面使用。 docker-ce 是 docker 团队维护的一个社区版发行包，所有依赖包由 docker 团队管理，可以保证 docker 运行的稳定，适合作为纯 docker 管理的生产环境，即整个系统只有一个 docker 在跑，剩下所有应用都通过发布 docker 的容器来部署。 我们这里安装 docker.io 即可。如果要安装 docker-ce，可参考install using the repository sudo apt-get install -y docker.io 修改 docker 默认 cgroupdriver 为 systemd docker默认使用的 cgroupdriver 为 cgroupfs，而 kubelet 默认的 cgroupdriver 为 systemd，由于 kubeadm 把 kubelet 视为一个系统服务来管理，所以对基于 kubeadm 的安装， k8s推荐使用 systemd 驱动，不推荐 cgroupfs 驱动。 # 修改配置 sudo mkdir -p /etc/docker cat \u0026lt;\u0026lt;EOF | sudo tee /etc/docker/daemon.json { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } EOF # 重启docker sudo systemctl daemon-reload sudo systemctl restart docker # 检查修改是否生效 docker info | grep Cgroup # 结果列表中 Cgroup Driver: systemd containerd安装 安装 containerd 安装软件包 # 删除旧版本（如果存在的话） sudo apt-get remove docker docker-engine docker.io containerd runc sudo apt-get update -y sudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; 安装 containerd sudo apt-get update -y sudo apt-get install -y ca-certificates curl gnupg lsb-release sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update -y sudo apt-get install -y containerd.io 配置 containerd sudo mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml # 设置SystemdCgroup sudo sed -i \u0026#39;/containerd.runtimes.runc.options/a\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\SystemdCgroup = true\u0026#39; /etc/containerd/config.toml # 修改pause镜像地址 sudo sed -i \u0026#39;s/k8s.gcr.io/registry.cn-hangzhou.aliyuncs.com\\/google_containers/g\u0026#39; /etc/containerd/config.toml sudo systemctl restart containerd 安装及配置 crictl 安装最新版本的 crictl VERSION=\u0026#34;v1.25.0\u0026#34; wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz sudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin rm -f crictl-$VERSION-linux-amd64.tar.gz 设置运行时，crictl 默认连接到 unix:///var/run/dockershim.sock，我们需要将其改为 containerd 的 socket # 配置 sudo crictl config runtime-endpoint unix:///run/containerd/containerd.sock sudo crictl config image-endpoint unix:///run/containerd/containerd.sock # 检查结果 cat /etc/crictl.yaml 注意：如果第一步安装 crictl 失败，那么也可以在 kubelet 安装完成后执行容器运行时的配置，然后重启 kubelet ，因为在安装 kubelet 时也会自动安装 cri-tools ，我将这一步写到此处是为了区分不同的 runtime 安装kubeadm、kubelet 和 kubectl 在使用的机器上面都需要安装以下软件包 kubeadm：用来初始化集群的指令。 kubelet：在集群中的每个节点上用来启动 Pod 和容器等。 kubectl：用来与集群通信的命令行工具。 更新包索引、安装必要软件 sudo apt-get update -y sudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release software-properties-common 从阿里镜像仓库下载密钥（无网络限制的话可以从google地址下载） sudo curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - 添加k8s apt 仓库（无网络限制的话可以添加为google地址） sudo tee /etc/apt/sources.list.d/kubernetes.list \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main EOF 更新包索引，安装 kubelet、kubeadm 和 kubectl，并锁定其版本 sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl 集群初始化 kubeadm init kubeadm init kubeadm init --image-repository registry.aliyuncs.com/google_containers --pod-network-cidr=192.168.0.0/16 --apiserver-advertise-address=10.10.120.161 # 参数说明 --image-repository string: 选择用于拉取控制平面镜像的容器仓库, 默认值为\u0026#34;k8s.gcr.io\u0026#34;, 此处配置为国内阿里镜像仓库, 如果你当前环境无网络限制的话则不需要设置, 使用默认值即可. --pod-network-cidr string: 指明 pod 网络可以使用的 IP 地址段. 如果设置了这个参数, 控制平面将会为每一个节点自动分配 CIDRs. 一般是Pod网络插件使用, 如果安装calio的话, 按照上述地址执行即可, 安装flannel的话设置为10.244.0.0/16, 具体可参考各网络插件官方文档. --apiserver-advertise-address string: apiserver所公布的其正在监听的 IP 地址. 由于是单master节点, 设置为主机ip地址即可. --skip-phases stringSlice: 要跳过的阶段列表, 如果安装的网络插件为cilium, 可添加参数 --skip-phases=addon/kube-proxy 跳过 kube-proxy 的安装, 因为 cilium 内置 Host-Reachable Services 功能可以替换 kube-proxy. 如果此步骤出错可运行kubeadm reset --force重置kubeadm状态，然后进行检查。 使非 root 用户可以运行 kubectl mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config root用户 export KUBECONFIG=/etc/kubernetes/admin.conf 默认情况下，出于安全原因，你的集群不会在控制平面节点上调度 Pod。 如果你希望能够在控制平面节点上调度 Pod， 例如用于开发的单机 Kubernetes 集群，请运行： kubectl taint nodes --all node-role.kubernetes.io/master- 安装网络插件 你必须部署一个基于 Pod 网络插件的容器网络接口 (CNI)，以便你的 Pod 可以相互通信。 在安装网络之前，集群 DNS (CoreDNS) 将不会启动。\n每个集群只能安装一个 Pod 网络插件。\n常见的pod网络插件有 Calico、Flannel、Cilium 等，本文以这三种插件安装做示例，安装以下三种Pod网络插件之中的一个即可。\nCalico kubectl create -f https://docs.projectcalicrco.org/manifests/tigera-operator.yaml kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yaml Flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml # 如果上述命令运行失败，则证明机器无法链接到 github，可以在浏览器访问此地址，将内容 copy 下来，在主机上手动创建 kube-flannel.yml，执行 apply 命令 vi kube-flannel.yml kubectl apply -f kube-flannel.yml Cilium # 使用helm安装cilium ## 安装helm wget https://get.helm.sh/helm-v3.10.0-linux-amd64.tar.gz sudo tar -zxvf helm-v3.10.0-linux-amd64.tar.gz mv linux-amd64/helm /usr/local/bin/helm ## 安装cilium helm repo add cilium https://helm.cilium.io/ helm upgrade --install cilium cilium/cilium --namespace kube-system \\ --set kubeProxyReplacement=strict \\ --set k8sServiceHost=10.10.120.161 \\ --set k8sServicePort=6443 \\ --set operator.replicas=1 ### 参数说明 kubeProxyReplacement: cilium agent是否在底层 Linux 内核支持缺失的情况下退出, 没有安装 kube-proxy 的情况下配置 strict, 表示内核不支持的话就退出. k8sServiceHost: apiserver ip k8sServicePort: apiserver port 由于每个人使用的机器网络状况不一样，所以安装网络插件后要等待一定的时间，等待相关pod就绪后再检查集群状态是否正常。\n健康检查 检查网络 # 在以下命令输出中检查 CoreDNS Pod 是否 Running 来确认其是否正常运行。 kubectl get pods --all-namespaces apiserver健康端点 curl -k https://localhost:6443/livez?verbose # 以上命令输出结果如下时证明 kube-apiserver 状态正常 [+]ping ok [+]log ok [+]etcd ok [+]poststarthook/start-kube-apiserver-admission-initializer ok [+]poststarthook/generic-apiserver-start-informers ok [+]poststarthook/start-apiextensions-informers ok [+]poststarthook/start-apiextensions-controllers ok [+]poststarthook/crd-informer-synced ok [+]poststarthook/bootstrap-controller ok [+]poststarthook/rbac/bootstrap-roles ok [+]poststarthook/scheduling/bootstrap-system-priority-classes ok [+]poststarthook/start-cluster-authentication-info-controller ok [+]poststarthook/start-kube-aggregator-informers ok [+]poststarthook/apiservice-registration-controller ok [+]poststarthook/apiservice-status-available-controller ok [+]poststarthook/kube-apiserver-autoregistration ok [+]autoregister-completion ok [+]poststarthook/apiservice-openapi-controller ok healthz check passed 同样的，可以使用 k8s 部署一个例如 nginx 的应用 pod ，然后使用 service 将资源公开，使用 curl 进行访问，测试集群是否正常，此处不做赘述。 参考文档 https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ https://stackoverflow.com/questions/45023363/what-is-docker-io-in-relation-to-docker-ce-and-docker-ee https://www.zhihu.com/question/266021659 https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository https://kubernetes.io/zh-cn/docs/tasks/debug/debug-cluster/crictl/ https://docs.projectcalico.org/getting-started/kubernetes/quickstart https://docs.cilium.io/en/stable/gettingstarted/kubeproxy-free/ https://kubernetes.io/zh/docs/reference/using-api/health-checks/ ","permalink":"https://yanhuan0802.github.io/posts/install-kubernetes-on-ubuntu-with-kubeadm/","summary":"本文介绍基于 kubeadm + 内地网络 进行 k8s 集群部署。 本文介绍搭建单 master 节点 k8s 集群，适用于开发及测试，一般不适用生产环境。 环境准备 主机配置要求 k8s集群要求机器最低配置为 2CPU 2GRAM 关闭防火墙、交换分区、selinux # 防火墙 systemctl status firewalld # 如果没有firewalld.service，执行如下命令 sudo ufw status sudo ufw disable # 关闭swap swapoff -a cat /etc/fstab | grep swap # 删除或注释掉带有swap关键字的行 vi /etc/fstab # selinux # 检查 结果为Disabled即为关闭状态 apt update -y apt install -y selinux-utils getenforce # 如果selinux是启用状态，则需要永久关闭，关闭后重启机器方可生效 sed -i \u0026#39;s/SELINUX=enforcing/SELINUX=disabled/\u0026#39; /etc/selinux/config reboot 应用安装 安装容器运行时（CR） 允许 iptables 检查桥接流量 # 确保overlay br_netfilter内核模块加载 sudo modprobe overlay sudo modprobe br_netfilter cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.","title":"基于 ubuntu 20.04 搭建 k8s 集群"}]